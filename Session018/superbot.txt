import pandas as pd
import numpy as np
import argparse 
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, f1_score
import warnings
import pickle
import os

warnings.filterwarnings('ignore')

def auto_select_target(df):
    # Automatically select the target column as the numeric column with the most unique values
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if not numeric_cols.any():
        raise ValueError("No numeric columns found in the DataFrame.")
    
    target_column = numeric_cols[0]  # Default to the first numeric column
    for col in numeric_cols:
        if df[col].nunique() > df[target_column].nunique():
            target_column = col
    return target_column

def preprocess_data(df, target_column):
    X = df.drop(columns=[target_column])
    y = df[target_column]
    return X, y

def explore_data(df):
    print("\n--- Data Exploration ---")
    print("Basic insights and patterns from the dataset:")
    print(f"Dataset contains {df.shape[0]} rows and {df.shape[1]} columns.")
    print("Here are some interesting stats from the data:")
    print(df.describe())
    print("\nTop 5 most common values in each categorical column:")
    for col in df.select_dtypes(include=['object']):
        print(f"{col}:")
        print(df[col].value_counts().head(), "\n")

def print_human_readable_results(metrics, model_type, is_classification):
    print("\n--- Human Comprehendable Results ---")
    print(f"Model Type: {model_type}")

    if is_classification:
        print(f"Accuracy: {metrics['Accuracy']:.2f}")
        print(f"F1 Score: {metrics['F1 Score']:.2f}")
    else:
        print(f"Mean Squared Error (MSE): {metrics['MSE']:.2f}")
        print(f"Mean Absolute Error (MAE): {metrics['MAE']:.2f}")
        print(f"R² Score: {metrics['R²']:.2f}")

        # Explain R² in plain English
        if metrics['R²'] > 0.75:
            print("This model explains most of the variability in the data.")
        elif 0.5 < metrics['R²'] <= 0.75:
            print("The model explains a moderate amount of the variability in the data.")
        else:
            print("The model does not explain much of the variability in the data. Consider improving the model.")

def analyze_data(df):
    # Check for empty DataFrame
    if df.empty:
        print("The DataFrame is empty.")
        return

    # Print initial information about the dataset
    print("Dataset loaded with {} rows and {} columns.".format(df.shape[0], df.shape[1]))
    print("\nDataset summary:")
    print(df.describe(include='all'))  # Show summary of all data types
    print("\nData Types:")
    print(df.dtypes)

    # Convert all object types to numeric where possible
    for col in df.columns:
        if df[col].dtype == 'object':
            df[col] = pd.to_numeric(df[col], errors='coerce')

    # Handle missing values
    df.fillna(0, inplace=True)

    # Automatically select target column
    target_column = auto_select_target(df)
    print(f"\nAutomatically selected target column for analysis: {target_column}")

    X, y = preprocess_data(df, target_column)

    # Identify numerical and categorical columns
    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()

    print("\nNumerical columns:", numerical_cols)
    print("Categorical columns:", categorical_cols)

    # Check if the target variable is categorical
    is_classification = y.dtype == 'object' or len(y.unique()) < 10  # Heuristic for classification

    # Explore the data for patterns and learnings
    explore_data(df)

    # Create a Column Transformer for preprocessing
    transformers = []
    if numerical_cols:
        transformers.append(('num', StandardScaler(), numerical_cols))
    if categorical_cols:
        transformers.append(('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols))

    preprocessor = ColumnTransformer(transformers=transformers)

    # Define models to evaluate
    model_results = {}
    if is_classification:
        models = {
            'Decision Tree': DecisionTreeClassifier(random_state=42),
            'Random Forest': RandomForestClassifier(random_state=42)
        }
    else:
        models = {
            'Linear Regression': LinearRegression(),
            'Decision Tree': DecisionTreeRegressor(random_state=42),
            'Random Forest': RandomForestRegressor(random_state=42)
        }

    # Loop through models and evaluate each one
    for model_name, model in models.items():
        # Create a pipeline with preprocessing and the selected model
        pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])

        # Split the data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Fit the pipeline
        pipeline.fit(X_train, y_train)

        # Make predictions
        y_pred = pipeline.predict(X_test)

        # Calculate metrics based on the task type
        if is_classification:
            accuracy = accuracy_score(y_test, y_pred)
            f1 = f1_score(y_test, y_pred, average='weighted')  # F1 score for multi-class
            model_results[model_name] = {'Accuracy': accuracy, 'F1 Score': f1}
        else:
            mse = mean_squared_error(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            model_results[model_name] = {'MSE': mse, 'MAE': mae, 'R²': r2}

    # Find the best model based on relevant metrics
    if is_classification:
        best_model_name = max(model_results, key=lambda x: model_results[x]['F1 Score'])
    else:
        best_model_name = max(model_results, key=lambda x: model_results[x]['R²'])
    
    best_model_results = model_results[best_model_name]

    # Print results for each model
    for name, metrics in model_results.items():
        print(f"{name}: ", end="")
        if is_classification:
            print(f"Accuracy={metrics['Accuracy']:.2f}, F1 Score={metrics['F1 Score']:.2f}")
        else:
            print(f"MSE={metrics['MSE']:.2f}, MAE={metrics['MAE']:.2f}, R²={metrics['R²']:.2f}")

    if is_classification:
        print(f"\nBest Model: {best_model_name} with F1 score of {best_model_results['F1 Score']:.2f}")
    else:
        print(f"\nBest Model: {best_model_name} with R² score of {best_model_results['R²']:.2f}")

def main():
    # Command-line argument parsing
    parser = argparse.ArgumentParser(description="Machine Learning Model")
    parser.add_argument('file', type=str, help='Path to the CSV file')
    args = parser.parse_args()

    input_path = args.file
    #if it is file
    if os.path.isfile(input_path):
        df = pd.read_csv(input_path)# Load your dataset here
        print(f"Dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns.")
        model=analyze_data(df)
        #creating pickle file
        PICKLE_FILE = input_path.replace('.csv','_model.pkl')

        with open(PICKLE_FILE,'wb') as f:
            pickle.dump(model, f)
    #if it is folder
    elif os.path.isdir(input_path):
        for file in os.listdir(input_path):
            if file.endswith('.csv'):
                print(file)
                file_path = os.path.join(input_path,file)
                df = pd.read_csv(file_path)
                model = analyze_data(df)
                #creating pickle file
                PICKLE_FILE = file_path.replace('.csv','_model.pkl')
                with open(PICKLE_FILE,'wb') as f:
                    pickle.dump(model, f)
            else:
                print("csv file not found in the folder")

if __name__ == "__main__":
    main()
